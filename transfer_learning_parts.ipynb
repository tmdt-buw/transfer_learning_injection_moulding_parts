{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "First import all the necessary libraries from Pytorch, Scikit-Learn, Numpy, Pandas, and Matplotlib. Note that these libraries need to be installed first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "seed = 1269\n",
    "\n",
    "def set_seeds(seed):    \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load experimental data from the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/lego_brick_data.csv\", sep=';')\n",
    "data = data[['Legobaustein', 'Testlauf', 'Max_Deformation', 'Nachdruck', 'Nachdruckzeit', 'Schmelzetemperatur', 'Wandtemperatur', 'Kuehlzeit', 'Volumenstrom']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Difference of 2 lego bricks\n",
    "Visualize the difference between 2 lego bricks (e.g. 4x2 and 4x1) by plotting the quality criterion (maximum deformation) to one parameter (Nachdruck). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_list = ['Max_Deformation', 'Nachdruck', 'Nachdruckzeit', 'Schmelzetemperatur', 'Wandtemperatur', 'Kuehlzeit', 'Volumenstrom']\n",
    "\n",
    "part_1_name = '4x2_Lego'\n",
    "part_2_name = '4x1_Lego'\n",
    "\n",
    "lego_1 = data.loc[data['Legobaustein'] == part_1_name][feature_list]\n",
    "lego_2 = data.loc[data['Legobaustein'] == part_2_name][feature_list]\n",
    "\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.set_title(part_1_name, fontsize=15)\n",
    "ax2.set_title(part_2_name, fontsize=15)\n",
    "\n",
    "ax1.set_ylim(0.4, 1.5)\n",
    "ax2.set_ylim(0.4, 1.5)\n",
    "\n",
    "ax1.xaxis.set_tick_params(labelsize=15)\n",
    "ax1.yaxis.set_tick_params(labelsize=15)\n",
    "ax2.xaxis.set_tick_params(labelsize=15)\n",
    "ax2.yaxis.set_tick_params(labelsize=15)\n",
    "\n",
    "sns.regplot(lego_1['Nachdruck'], lego_1['Max_Deformation'], color='steelblue', ax=ax1)\n",
    "sns.regplot(lego_1['Nachdruck'], lego_1['Max_Deformation'], color='steelblue', ax=ax2, label=part_1_name)\n",
    "sns.regplot(lego_2['Nachdruck'], lego_2['Max_Deformation'], color='darkorange', ax=ax2, label=part_2_name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data dictionaries\n",
    "Split the data into separate sets (one set per lego brick). Each data set is split into training set (~57 records), validation set (~8 records), and training set (~12 records). In addition, we scale the features for better training of the neural network regressor.\n",
    "The method returns a dictionary containing all the data sets for each lego brick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dict(train_percentage, val_percentage, test_percentage):\n",
    "    \"\"\"\n",
    "    This method loads the data for all parts. \n",
    "    :return: A dictionary containing the raw and pre processed data used to fit the model.\n",
    "    \"\"\"\n",
    "    set_seeds(1273)\n",
    "    scale = StandardScaler()\n",
    "    data_path = \"data/lego_brick_data.csv\"\n",
    "    data_dict = {}\n",
    "    parameters = np.array(['Nachdruck', 'Nachdruckzeit', 'Schmelzetemperatur', 'Wandtemperatur', 'Kuehlzeit', 'Volumenstrom'])\n",
    "    quality_criteria = np.array(['Max_Deformation'])\n",
    "    n_inputs = len(parameters)\n",
    "    n_outputs = len(quality_criteria)\n",
    "\n",
    "    data = pd.read_csv(data_path, sep=';')\n",
    "    \n",
    "    # iterate over all lego bricks (e.g. 4x2, 3x1, 4x1)\n",
    "    for value in data['Legobaustein'].unique():\n",
    "        mask = data['Legobaustein'] == value\n",
    "        data_reduced = data[mask]\n",
    "        data_reduced = data_reduced[np.append(parameters, quality_criteria)]\n",
    "        data_dict[value] = {}\n",
    "\n",
    "        data_reduced = shuffle(data_reduced, random_state=seed)\n",
    "\n",
    "        # data is divided into training, covalidation and testing sets\n",
    "        data_n = len(data_reduced)\n",
    "\n",
    "        train_data = data_reduced.values[0:int(data_n * train_percentage)]\n",
    "\n",
    "        cval_data = data_reduced.values[\n",
    "                    int(data_n * train_percentage):int(data_n * (train_percentage + val_percentage))]\n",
    "\n",
    "        test_data = data_reduced.values[int(data_n * (train_percentage + val_percentage)):int(\n",
    "            data_n * (train_percentage + val_percentage + test_percentage))]\n",
    "\n",
    "        # Pytorch Tensor objects for the data sets are created with X data scaled fitted to train data\n",
    "        data_dict[value][\"train_x\"] = torch.from_numpy(scale.fit_transform(train_data[:, 0:n_inputs])).float()\n",
    "        data_dict[value][\"train_y\"] = torch.from_numpy(train_data[:, n_inputs:n_inputs + n_outputs]).float()\n",
    "\n",
    "        data_dict[value][\"val_x\"] = torch.from_numpy(scale.transform(cval_data[:, 0:n_inputs])).float()\n",
    "        data_dict[value][\"val_y\"] = torch.from_numpy(cval_data[:, n_inputs:n_inputs + n_outputs]).float()\n",
    "\n",
    "        data_dict[value][\"test_x\"] = torch.from_numpy(scale.transform(test_data[:, 0:n_inputs])).float()\n",
    "        data_dict[value][\"test_y\"] = torch.from_numpy(test_data[:, n_inputs:n_inputs + n_outputs]).float()\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Print data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = load_data_dict(0.75, 0.15, 0.1)\n",
    "print('Number of parts: {}'.format(len(data_dict)))\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Methods to create and train neural networks\n",
    "At first, we define the class for the neural network model using inherited attributes from the Module class in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 45)\n",
    "        self.fc2 = nn.Linear(45, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the upcoming experiments, we use DataLoaders from Pytorch to load the respective training or validation set. The fraction denotes how much proportion of the training set is to be loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(data_dict, part, fraction):\n",
    "    train_x = data_dict[part]['train_x'] \n",
    "    train_y = data_dict[part]['train_y']\n",
    "    train_x = train_x[:int(fraction * len(train_x))]\n",
    "    train_y = train_y[:int(fraction * len(train_y))]\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    trainloader = DataLoader(train_dataset, batch_size=10)\n",
    "\n",
    "    val_x = data_dict[part]['val_x']\n",
    "    val_y = data_dict[part]['val_y']\n",
    "    val_dataset = TensorDataset(val_x, val_y)\n",
    "    valloader = DataLoader(val_dataset, batch_size=10)\n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the method for the training of the model and its validation. In the process the model will train through every batch until the validation loss has not improved for the amount of epochs specified in patience. The trained model and the amount of epochs required are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, trainloader, valloader, num_epochs, patience=100, logging=True, early_stopping=True):\n",
    "\n",
    "    # early stop parameters\n",
    "    delta = 0.00001\n",
    "    stop = False\n",
    "    best_loss = None\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # per epoch the model is trained on all batches of the training dataset\n",
    "        num_batches = 0\n",
    "        batch_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            model.train()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        epoch_loss = batch_loss / num_batches\n",
    "        \n",
    "        # the model is then evaluated on all batches of the validation dataset\n",
    "        num_batches = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            model.eval()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        val_loss = val_loss / num_batches\n",
    "\n",
    "        \n",
    "        # Early stoppging: training stops if no improvement in validation loss\n",
    "        if early_stopping:\n",
    "            if best_loss is None:\n",
    "                best_loss = val_loss\n",
    "            elif val_loss < best_loss - delta:\n",
    "                best_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    stop = True\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            if logging:\n",
    "                print('Train Epoch: {} \\tTrain_Loss: {:.6f} \\t Validation_Loss: {:.6f}, Best_Loss: {:.6f}'.format(epoch, epoch_loss, val_loss, best_loss))\n",
    "\n",
    "        if stop:\n",
    "            if logging:\n",
    "                print(\"EARLY STOPPING at epoch {}\".format(epoch))\n",
    "            break\n",
    "    return model, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the method for evaluating the model's accuracy on a test set. The metric used is the coefficient of determination R² from the scikit-learn library. It is define as follows:\n",
    "<img src=\"images/r2_score_formular.png\" width=\"600\">\n",
    "The R² score is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "        net_out = model(test_x)\n",
    "        score = r2_score(net_out.cpu().numpy(), test_y.cpu().numpy())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (lego brick to lego brick)\n",
    "\n",
    "In the following, we implement transfer learning by fitting a the network on one dateset and finetuning the network on a second data set. \n",
    "\n",
    "First, we load the data and set the device to gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_dict = load_data_dict(0.75, 0.1, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on first lego brick (4x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '4x2_Lego'\n",
    "set_seeds(seed)\n",
    "trainloader, valloader = get_loader(data_dict, part, 1.0)\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "criterion = nn.L1Loss()\n",
    "model, part_1_epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000)\n",
    "\n",
    "test_x, test_y = data_dict[part]['test_x'], data_dict[part]['test_y']\n",
    "model.eval()\n",
    "part_1_test_score = evaluate_model(model, test_x, test_y)\n",
    "print('*' * 30)\n",
    "print('Test score part 1: {:.4f}'.format(part_1_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer the trained model to second lego brick (by continuing the training)\n",
    "Note that we use only **20%** of the training data for the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3x1_Lego'\n",
    "set_seeds(seed)\n",
    "trainloader, valloader = get_loader(data_dict, part, 0.2)\n",
    "\n",
    "model.train()\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "model, part_2_tl_epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000)\n",
    "\n",
    "test_x, test_y = data_dict[part]['test_x'], data_dict[part]['test_y']\n",
    "model.eval()\n",
    "part_2_tl_test_score = evaluate_model(model, test_x, test_y)\n",
    "print('*' * 30)\n",
    "print('Test score part 2 (finetune): {:.4f}'.format(part_2_tl_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For comparison: training a model for the second lego brick from scratch\n",
    "Note that we re-initialize the network and that we use only **20%** of the training data for the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "part = '3x1_Lego'\n",
    "set_seeds(seed)\n",
    "trainloader, valloader = get_loader(data_dict, part, 0.2)\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "model, part_2_epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000)\n",
    "\n",
    "test_x, test_y = data_dict[part]['test_x'], data_dict[part]['test_y']\n",
    "model.eval()\n",
    "part_2_test_score = evaluate_model(model, test_x, test_y)\n",
    "print('*' * 30)\n",
    "print('Test score part 2 (from scratch): {:.4f}'.format(part_2_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results\n",
    "Comparison of training on part 1, transfer (finetuning) on lego brick 2, and training on lego brick 2 from scratch. The plots show the R² scores as well as the number of epochs used for training. Note that for lego brick 2, only 20% of the data was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.set_title('Test Score', fontsize=15)\n",
    "ax2.set_title('Number of Epochs', fontsize=15)\n",
    "\n",
    "ax1.set_ylim(0.7, 1.0)\n",
    "\n",
    "ax1.xaxis.set_tick_params(labelsize=15)\n",
    "ax1.yaxis.set_tick_params(labelsize=15)\n",
    "ax2.xaxis.set_tick_params(labelsize=15)\n",
    "ax2.yaxis.set_tick_params(labelsize=15)\n",
    "\n",
    "\n",
    "ax1.bar([\"Part 1\", \"Part 2 - Single\", \"Part 2 - Transfer\"], [part_1_test_score, part_2_test_score, part_2_tl_test_score], color='steelblue')\n",
    "ax2.bar([\"Part 1\", \"Part 2 - Single\", \"Part 2 - Transfer\"], [part_1_epochs, part_2_epochs, part_2_tl_epochs], color='darkorange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (lego brick to lego brick) with incremental scores\n",
    "\n",
    "In the following, we implement the same methods with additional evaluation. We train the model for the second lego brick on different data sizes to compare the performance between transfer learning and training from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on first lego brick (4x2) and finetune the network to the second lego brick (3x1): training on different proportions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '4x2_Lego'\n",
    "set_seeds(seed)\n",
    "trainloader, valloader = get_loader(data_dict, part, 1.0)\n",
    "print('Train on first lego brick')\n",
    "model = Net()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "criterion = nn.L1Loss()\n",
    "model, epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000, logging=False)\n",
    "\n",
    "\n",
    "part = '3x1_Lego'\n",
    "set_seeds(seed)\n",
    "print('Transfer on second lego brick')\n",
    "fractions = np.arange(0.1, 1.01, 0.1)\n",
    "scores_frac = []\n",
    "bck_model = copy.deepcopy(model)\n",
    "for frac in fractions:\n",
    "    trainloader, valloader = get_loader(data_dict, part, frac)\n",
    "\n",
    "    model = copy.deepcopy(bck_model)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "    model, epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000, logging=False)\n",
    "\n",
    "    test_x, test_y = data_dict[part]['test_x'], data_dict[part]['test_y']\n",
    "    model.eval()\n",
    "    test_score = evaluate_model(model, test_x, test_y)\n",
    "    print('Test score for fraction {:.1f}: {:.4f}'.format(frac, test_score))\n",
    "    scores_frac.append(max(-2.0, test_score))\n",
    "transfer_scores_frac = scores_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For comparison: train the network for the second lego brick from scratch, again on different proportions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "part = '3x1_Lego'\n",
    "set_seeds(seed)\n",
    "print('Train on second lego brick from scratch')\n",
    "fractions = np.arange(0.1, 1.01, 0.1)\n",
    "scores_frac = []\n",
    "for frac in fractions:\n",
    "    trainloader, valloader = get_loader(data_dict, part, frac)\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "    model, epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000, logging=False)\n",
    "\n",
    "    test_x, test_y = data_dict[part]['test_x'], data_dict[part]['test_y']\n",
    "    model.eval()\n",
    "    test_score = evaluate_model(model, test_x, test_y)\n",
    "    print('Test score for fraction {:.1f}: {:.4f}'.format(frac, test_score))\n",
    "    scores_frac.append(max(-2.0, test_score))\n",
    "control_scores_frac = scores_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results\n",
    "Comparison of transfer (finetuning) on lego brick 2 and training on lego brick 2 from scratch. The plots show the R² scores over varying data sizes (proportions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Test Score', fontsize=15)\n",
    "\n",
    "xaxis = np.arange(0.1, 1.01, 0.1)\n",
    "ax.xaxis.set_tick_params(labelsize=15)\n",
    "ax.yaxis.set_tick_params(labelsize=15)\n",
    "ax.set_xlabel('Proportion of training data', fontsize=15)\n",
    "ax.set_ylabel('Test score', fontsize=15)\n",
    "ax.plot(xaxis, transfer_scores_frac, color='steelblue', label='With Tansfer', linewidth=2)\n",
    "ax.plot(xaxis,control_scores_frac, color='darkorange', label='Without Transfer', linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning for 3 lego bricks\n",
    "\n",
    "In the following, we apply two-time-transfer from lego brick \"3x2\" to \"4x2\" to \"3x1\" and print the incremental results for the last transfer. This is an extension of the previous transfer (from \"4x2\" to \"3x1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "part = '3x2_Lego'\n",
    "set_seeds(seed)\n",
    "trainloader, valloader = get_loader(data_dict, part, 1.0)\n",
    "\n",
    "print('Train on first lego brick')\n",
    "model = Net()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "criterion = nn.L1Loss()\n",
    "model, epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000, logging=False)\n",
    "\n",
    "\n",
    "part = '4x2_Lego'\n",
    "set_seeds(seed)\n",
    "trainloader, valloader = get_loader(data_dict, part, 1.0)\n",
    "\n",
    "print('Transfer on second lego brick')\n",
    "model.train()\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "model, epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000, logging=False)\n",
    "\n",
    "part = '3x1_Lego'\n",
    "set_seeds(seed)\n",
    "print('Transfer on third lego brick')\n",
    "fractions = np.arange(0.1, 1.01, 0.1)\n",
    "scores_frac = []\n",
    "bck_model = copy.deepcopy(model)\n",
    "for frac in fractions:\n",
    "    trainloader, valloader = get_loader(data_dict, part, frac)\n",
    "\n",
    "    model = copy.deepcopy(bck_model)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001, amsgrad=True)\n",
    "    model, epochs = train_model(model, optimizer, criterion, trainloader, valloader, num_epochs=1000, logging=False)\n",
    "\n",
    "    test_x, test_y = data_dict[part]['test_x'], data_dict[part]['test_y']\n",
    "    model.eval()\n",
    "    test_score = evaluate_model(model, test_x, test_y)\n",
    "    print('Test score for fraction {:.1f}: {:.4f}'.format(frac, test_score))\n",
    "    scores_frac.append(max(-2.0, test_score))\n",
    "double_transfer_scores_frac = scores_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results\n",
    "Comparison of the 3 approaches: \n",
    "-  training from scratch (on 3x1)\n",
    "-  1-time-transfer with finetuning (4x2 to 3x1)\n",
    "-  2-time-transfer with finetuning (3x2 to 4x2 to 3x1)\n",
    "\n",
    "The plots show the R² scores over varying data sizes (proportions) for the last lego brick (3x1). The red line indicatas a threshold of the score at 0.95. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Test Score', fontsize=15)\n",
    "\n",
    "xaxis = np.arange(0.1, 1.01, 0.1)\n",
    "ax.xaxis.set_tick_params(labelsize=15)\n",
    "ax.yaxis.set_tick_params(labelsize=15)\n",
    "ax.set_xlabel('Proportion of training data', fontsize=15)\n",
    "ax.set_ylabel('Test score', fontsize=15)\n",
    "ax.plot(xaxis, control_scores_frac, color='darkorange', label='Without Transfer', linewidth=2)\n",
    "ax.plot(xaxis, transfer_scores_frac, color='steelblue', label='With 1 Tansfer', linewidth=2)\n",
    "ax.plot(xaxis, double_transfer_scores_frac, color='darkgreen', label='With 2 Transfer', linewidth=2)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', linewidth=1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mulitple transfer (3-time transfer with 4 lego bricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purpose: creating 24 different lists (permutations) out of 4 lego bricks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_list = ['4x2_Lego', '4x1_Lego', '3x1_Lego','3x2_Lego']\n",
    "for idx, perm in enumerate(permutations(part_list)):\n",
    "    print(idx, perm)\n",
    "    # for seed in np.arange(10):        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the average proportion of data records needed for training to reach a test score of 0.95: \n",
    "1.  Training from scratch (part 1)\n",
    "2.  First transfer (part 2)\n",
    "3.  Second transfer (part 3)\n",
    "4.  Third transfer (part 4)\n",
    "\n",
    "Note that the averages are calculated across all permutations, whereas each experiment is conducted 10 times (with 10 different seeds). The conducted experiments are not part of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for filename in os.listdir('Results/transfer_finetune_permutations'):\n",
    "    if filename[0:35] == 'transfer_finetune_fracs_permutation':\n",
    "        scores.append(np.loadtxt('Results/transfer_finetune_permutations/{}'.format(filename)))\n",
    "scores = np.array(scores)\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "std_scores = np.std(scores, axis=0)\n",
    "print(mean_scores)\n",
    "\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "ax = fig.add_subplot(111)\n",
    "x_axis = ['Part 1', 'Part 2', 'Part 3', 'Part 4']\n",
    "ax.set_ylabel('Proportion', fontsize=15)\n",
    "ax.xaxis.set_tick_params(labelsize=15)\n",
    "ax.yaxis.set_tick_params(labelsize=15)\n",
    "ax.plot(x_axis, mean_scores, marker='o', markersize=10, linewidth=3)\n",
    "ax.fill_between(x_axis, mean_scores + std_scores, mean_scores - std_scores, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
